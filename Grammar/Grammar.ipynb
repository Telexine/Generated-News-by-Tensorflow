{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
<<<<<<< HEAD
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-gpu==2.0.0-beta1 in c:\\users\\telexine\\.conda\\envs\\tf_beta\\lib\\site-packages (2.0.0b1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\telexine\\.conda\\envs\\tf_beta\\lib\\site-packages (from tensorflow-gpu==2.0.0-beta1) (1.1.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in c:\\users\\telexine\\.conda\\envs\\tf_beta\\lib\\site-packages (from tensorflow-gpu==2.0.0-beta1) (3.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\telexine\\.conda\\envs\\tf_beta\\lib\\site-packages (from tensorflow-gpu==2.0.0-beta1) (1.11.2)\n",
      "Requirement already satisfied: tb-nightly<1.14.0a20190604,>=1.14.0a20190603 in c:\\users\\telexine\\.conda\\envs\\tf_beta\\lib\\site-packages (from tensorflow-gpu==2.0.0-beta1) (1.14.0a20190603)\n",
      "Requirement already satisfied: gast>=0.2.0 in c:\\users\\telexine\\.conda\\envs\\tf_beta\\lib\\site-packages (from tensorflow-gpu==2.0.0-beta1) (0.2.2)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in c:\\users\\telexine\\.conda\\envs\\tf_beta\\lib\\site-packages (from tensorflow-gpu==2.0.0-beta1) (0.7.1)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\telexine\\.conda\\envs\\tf_beta\\lib\\site-packages (from tensorflow-gpu==2.0.0-beta1) (1.12.0)\n",
      "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019060502,>=1.14.0.dev2019060501 in c:\\users\\telexine\\.conda\\envs\\tf_beta\\lib\\site-packages (from tensorflow-gpu==2.0.0-beta1) (1.14.0.dev2019060501)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\telexine\\.conda\\envs\\tf_beta\\lib\\site-packages (from tensorflow-gpu==2.0.0-beta1) (1.1.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in c:\\users\\telexine\\.conda\\envs\\tf_beta\\lib\\site-packages (from tensorflow-gpu==2.0.0-beta1) (0.1.7)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in c:\\users\\telexine\\.conda\\envs\\tf_beta\\lib\\site-packages (from tensorflow-gpu==2.0.0-beta1) (1.22.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.14.5 in c:\\users\\telexine\\.conda\\envs\\tf_beta\\lib\\site-packages (from tensorflow-gpu==2.0.0-beta1) (1.16.4)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\telexine\\.conda\\envs\\tf_beta\\lib\\site-packages (from tensorflow-gpu==2.0.0-beta1) (0.33.4)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in c:\\users\\telexine\\.conda\\envs\\tf_beta\\lib\\site-packages (from tensorflow-gpu==2.0.0-beta1) (1.0.8)\n",
      "Requirement already satisfied: astor>=0.6.0 in c:\\users\\telexine\\.conda\\envs\\tf_beta\\lib\\site-packages (from tensorflow-gpu==2.0.0-beta1) (0.8.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\telexine\\.conda\\envs\\tf_beta\\lib\\site-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0-beta1) (41.0.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\telexine\\.conda\\envs\\tf_beta\\lib\\site-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0.0-beta1) (0.15.5)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\telexine\\.conda\\envs\\tf_beta\\lib\\site-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0.0-beta1) (3.1.1)\n",
      "Requirement already satisfied: h5py in c:\\users\\telexine\\.conda\\envs\\tf_beta\\lib\\site-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0-beta1) (2.9.0)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute '__version__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-47e6cb9d9ad6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute '__version__'"
     ]
=======
     "data": {
      "text/plain": [
       "'2.0.0-beta1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
>>>>>>> master
    }
   ],
   "source": [
    "try:\n",
<<<<<<< HEAD
    "  !pip install tensorflow-gpu==2.0.0-beta1\n",
=======
    "  # !pip install tensorflow-gpu==2.0.0-beta1\n",
>>>>>>> master
    "  pass\n",
    "except Exception:\n",
    "  pass\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import string\n",
<<<<<<< HEAD
    "import nltk\n",
=======
>>>>>>> master
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag \n",
    "porter = PorterStemmer()\n",
<<<<<<< HEAD
    "\n",
    "\n",
    "import _pickle as pickle\n",
    "\n",
=======
    "import nltk\n",
>>>>>>> master
    "# uncomment line below for the first time to download the nltk packages\n",
    "#nltk.download()\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tf.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TENSOR core\n",
    "os.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = '1'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = RegexpTokenizer(r\"[\\“\\‘\\\"\\,’”\\.]|[a-zA-Z]+\")\n",
    "stop_words = set(stopwords.words('english')) \n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "def remove_punc(txt):\n",
    "    return \"\".join([t for t in txt if t not in string.punctuation])\n",
    "# Split sentense to list and remove non text\n",
    "def tokenizeF(df):\n",
    "    return tk.tokenize(text=df.lower())\n",
    "#Lemmatizing to reduce inflectional forms \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def word_lemmatizer(txt):\n",
    "    #change to nounce\n",
    "    return[ lemmatizer.lemmatize(word,'n')  for word in txt  ]\n",
    "# Stem Words\n",
    "\n",
    "def word_stem(txt):\n",
    "    return [porter.stem(word) for word in txt]\n",
    "def stop_word(txt):\n",
    "    filtered_sentence = [] \n",
    "    for w in txt: \n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w) \n",
    "    return filtered_sentence \n",
    "\n",
    "#Convert word to grammar tag\n",
    "def pos_tagging(txt): \n",
    "    return    [x[1] for x in pos_tag(txt)]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =[]\n",
    "ar= pd.read_csv('../Datasets/articles1.csv')\n",
    "df.append(ar)\n",
    "ar = pd.read_csv('../Datasets/articles2.csv')\n",
    "df.append(ar)\n",
    "ar = pd.read_csv('../Datasets/articles3.csv')\n",
    "df.append(ar)\n",
    "df = pd.concat(df, ignore_index=True,sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WASHINGTON  —   Congressional Republicans have a new fear when it comes to their    health care lawsuit against the Obama administration: They might win. The incoming Trump administration could choose to no longer defend the executive branch against the suit, which challenges the administration’s authority to spend billions of dollars on health insurance subsidies for   and   Americans, handing House Republicans a big victory on    issues. But a sudden loss of the disputed subsidies could conceivably cause the health care program to implode, leaving millions of people without access to health insurance before Republicans have prepared a replacement. That could lead to chaos in the insurance market and spur a political backlash just as Republicans gain full control of the government. To stave off that outcome, Republicans could find themselves in the awkward position of appropriating huge sums to temporarily prop up the Obama health care law, angering conservative voters who have been demanding an end to the law for years. In another twist, Donald J. Trump’s administration, worried about preserving executive branch prerogatives, could choose to fight its Republican allies in the House on some central questions in the dispute. Eager to avoid an ugly political pileup, Republicans on Capitol Hill and the Trump transition team are gaming out how to handle the lawsuit, which, after the election, has been put in limbo until at least late February by the United States Court of Appeals for the District of Columbia Circuit. They are not yet ready to divulge their strategy. “Given that this pending litigation involves the Obama administration and Congress, it would be inappropriate to comment,” said Phillip J. Blando, a spokesman for the Trump transition effort. “Upon taking office, the Trump administration will evaluate this case and all related aspects of the Affordable Care Act. ” In a potentially   decision in 2015, Judge Rosemary M. Collyer ruled that House Republicans had the standing to sue the executive branch over a spending dispute and that the Obama administration had been distributing the health insurance subsidies, in violation of the Constitution, without approval from Congress. The Justice Department, confident that Judge Collyer’s decision would be reversed, quickly appealed, and the subsidies have remained in place during the appeal. In successfully seeking a temporary halt in the proceedings after Mr. Trump won, House Republicans last month told the court that they “and the  ’s transition team currently are discussing potential options for resolution of this matter, to take effect after the  ’s inauguration on Jan. 20, 2017. ” The suspension of the case, House lawyers said, will “provide the   and his future administration time to consider whether to continue prosecuting or to otherwise resolve this appeal. ” Republican leadership officials in the House acknowledge the possibility of “cascading effects” if the   payments, which have totaled an estimated $13 billion, are suddenly stopped. Insurers that receive the subsidies in exchange for paying    costs such as deductibles and   for eligible consumers could race to drop coverage since they would be losing money. Over all, the loss of the subsidies could destabilize the entire program and cause a lack of confidence that leads other insurers to seek a quick exit as well. Anticipating that the Trump administration might not be inclined to mount a vigorous fight against the House Republicans given the  ’s dim view of the health care law, a team of lawyers this month sought to intervene in the case on behalf of two participants in the health care program. In their request, the lawyers predicted that a deal between House Republicans and the new administration to dismiss or settle the case “will produce devastating consequences for the individuals who receive these reductions, as well as for the nation’s health insurance and health care systems generally. ” No matter what happens, House Republicans say, they want to prevail on two overarching concepts: the congressional power of the purse, and the right of Congress to sue the executive branch if it violates the Constitution regarding that spending power. House Republicans contend that Congress never appropriated the money for the subsidies, as required by the Constitution. In the suit, which was initially championed by John A. Boehner, the House speaker at the time, and later in House committee reports, Republicans asserted that the administration, desperate for the funding, had required the Treasury Department to provide it despite widespread internal skepticism that the spending was proper. The White House said that the spending was a permanent part of the law passed in 2010, and that no annual appropriation was required  —   even though the administration initially sought one. Just as important to House Republicans, Judge Collyer found that Congress had the standing to sue the White House on this issue  —   a ruling that many legal experts said was flawed  —   and they want that precedent to be set to restore congressional leverage over the executive branch. But on spending power and standing, the Trump administration may come under pressure from advocates of presidential authority to fight the House no matter their shared views on health care, since those precedents could have broad repercussions. It is a complicated set of dynamics illustrating how a quick legal victory for the House in the Trump era might come with costs that Republicans never anticipated when they took on the Obama White House.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['content'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = 1000\n",
    "df2 = df['content'].head(articles).apply(tokenizeF).apply( pos_tagging)\n",
    "gtag = flatten(df2.values.tolist())\n",
    "#del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['JJ',\n",
       " 'JJ',\n",
       " 'NNS',\n",
       " 'VBP',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'WRB',\n",
       " 'PRP',\n",
       " 'VBZ',\n",
       " 'TO',\n",
       " 'PRP$',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'PRP',\n",
       " 'MD',\n",
       " 'VB',\n",
       " '.',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'MD',\n",
       " 'VB',\n",
       " 'TO',\n",
       " 'DT',\n",
       " 'JJR',\n",
       " 'VB',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " ',',\n",
       " 'WDT',\n",
       " 'VBZ',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'NNP',\n",
       " 'VBZ',\n",
       " 'NN',\n",
       " 'TO',\n",
       " 'VB',\n",
       " 'NNS',\n",
       " 'IN',\n",
       " 'NNS',\n",
       " 'IN',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'NNS',\n",
       " 'IN',\n",
       " 'CC',\n",
       " 'NNS',\n",
       " ',',\n",
       " 'VBG',\n",
       " 'NN',\n",
       " 'VBZ',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'NNS',\n",
       " '.',\n",
       " 'CC',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'NNS',\n",
       " 'MD',\n",
       " 'RB',\n",
       " 'VB',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'TO',\n",
       " 'VB',\n",
       " ',',\n",
       " 'VBG',\n",
       " 'NNS',\n",
       " 'IN',\n",
       " 'NNS',\n",
       " 'IN',\n",
       " 'NN',\n",
       " 'TO',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'NNS',\n",
       " 'VBP',\n",
       " 'VBN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " '.',\n",
       " 'WDT',\n",
       " 'MD',\n",
       " 'VB',\n",
       " 'TO',\n",
       " 'VB',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'CC',\n",
       " 'VB',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'RB',\n",
       " 'IN',\n",
       " 'NNS',\n",
       " 'VBP',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " '.',\n",
       " 'TO',\n",
       " 'VB',\n",
       " 'RP',\n",
       " 'DT',\n",
       " 'NN',\n",
       " ',',\n",
       " 'NNS',\n",
       " 'MD',\n",
       " 'VB',\n",
       " 'PRP',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'VBG',\n",
       " 'JJ',\n",
       " 'NNS',\n",
       " 'TO',\n",
       " 'RB',\n",
       " 'VB',\n",
       " 'RP',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'NN',\n",
       " ',',\n",
       " 'VBG',\n",
       " 'JJ',\n",
       " 'NNS',\n",
       " 'WP',\n",
       " 'VBP',\n",
       " 'VBN',\n",
       " 'VBG',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'TO',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'NNS',\n",
       " '.',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " ',',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " '.',\n",
       " 'NN',\n",
       " 'NNP',\n",
       " 'NN',\n",
       " 'NN',\n",
       " ',',\n",
       " 'VBD',\n",
       " 'IN',\n",
       " 'VBG',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'NNS',\n",
       " ',',\n",
       " 'MD',\n",
       " 'VB',\n",
       " 'TO',\n",
       " 'VB',\n",
       " 'PRP$',\n",
       " 'JJ',\n",
       " 'NNS',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'NNS',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " '.',\n",
       " 'JJ',\n",
       " 'TO',\n",
       " 'VB',\n",
       " 'DT',\n",
       " 'RB',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " ',',\n",
       " 'VBZ',\n",
       " 'IN',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'CC',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'VBP',\n",
       " 'VBG',\n",
       " 'RP',\n",
       " 'WRB',\n",
       " 'TO',\n",
       " 'VB',\n",
       " 'DT',\n",
       " 'NN',\n",
       " ',',\n",
       " 'WDT',\n",
       " ',',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " ',',\n",
       " 'VBZ',\n",
       " 'VBN',\n",
       " 'VBN',\n",
       " 'IN',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'IN',\n",
       " 'JJS',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'NNS',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'NNS',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'NN',\n",
       " 'NN',\n",
       " '.',\n",
       " 'PRP',\n",
       " 'VBP',\n",
       " 'RB',\n",
       " 'RB',\n",
       " 'JJ',\n",
       " 'TO',\n",
       " 'VB',\n",
       " 'PRP$',\n",
       " 'NN',\n",
       " '.',\n",
       " 'VB',\n",
       " 'VBN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'VBG',\n",
       " 'NN',\n",
       " 'VBZ',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'CC',\n",
       " 'NN',\n",
       " ',',\n",
       " 'PRP',\n",
       " 'MD',\n",
       " 'VB',\n",
       " 'JJ',\n",
       " 'TO',\n",
       " 'VB',\n",
       " ',',\n",
       " 'NNP',\n",
       " 'VBD',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " '.',\n",
       " 'NN',\n",
       " ',',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'NN',\n",
       " '.',\n",
       " 'CC',\n",
       " 'IN',\n",
       " 'VBG',\n",
       " 'NN',\n",
       " ',',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'MD',\n",
       " 'VB',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'CC',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'NNS',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'NN',\n",
       " '.',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'RB',\n",
       " 'NN',\n",
       " 'IN',\n",
       " ',',\n",
       " 'NN',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " '.',\n",
       " 'NN',\n",
       " 'VBD',\n",
       " 'IN',\n",
       " 'NN',\n",
       " 'NNS',\n",
       " 'VBD',\n",
       " 'DT',\n",
       " 'VBG',\n",
       " 'TO',\n",
       " 'VB',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'CC',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'VBD',\n",
       " 'VBN',\n",
       " 'VBG',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'NNS',\n",
       " ',',\n",
       " 'IN',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " ',',\n",
       " 'IN',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'NN',\n",
       " '.',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'NN',\n",
       " ',',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'NNP',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'MD',\n",
       " 'VB',\n",
       " 'VBN',\n",
       " ',',\n",
       " 'RB',\n",
       " 'VBD',\n",
       " ',',\n",
       " 'CC',\n",
       " 'DT',\n",
       " 'NNS',\n",
       " 'VBP',\n",
       " 'VBN',\n",
       " 'IN',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " '.',\n",
       " 'IN',\n",
       " 'RB',\n",
       " 'VBG',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NNS',\n",
       " 'IN',\n",
       " 'NN',\n",
       " '.',\n",
       " 'NN',\n",
       " 'VBD',\n",
       " ',',\n",
       " 'NN',\n",
       " 'NNS',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'VBD',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'PRP',\n",
       " 'VBP',\n",
       " 'CC',\n",
       " 'DT',\n",
       " 'NNP',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'RB',\n",
       " 'VBP',\n",
       " 'VBG',\n",
       " 'JJ',\n",
       " 'NNS',\n",
       " 'IN',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " ',',\n",
       " 'TO',\n",
       " 'VB',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NNP',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'NN',\n",
       " '.',\n",
       " ',',\n",
       " '.',\n",
       " 'VB',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " ',',\n",
       " 'NN',\n",
       " 'NNS',\n",
       " 'VBD',\n",
       " ',',\n",
       " 'MD',\n",
       " 'VB',\n",
       " 'VB',\n",
       " 'DT',\n",
       " 'CC',\n",
       " 'PRP$',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'TO',\n",
       " 'VB',\n",
       " 'IN',\n",
       " 'TO',\n",
       " 'VB',\n",
       " 'VBG',\n",
       " 'CC',\n",
       " 'TO',\n",
       " 'VB',\n",
       " 'VB',\n",
       " 'DT',\n",
       " 'NN',\n",
       " '.',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'NNS',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'VBZ',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'JJ',\n",
       " 'VBG',\n",
       " 'NNS',\n",
       " 'RB',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NNS',\n",
       " ',',\n",
       " 'WDT',\n",
       " 'VBP',\n",
       " 'VBD',\n",
       " 'DT',\n",
       " 'VBN',\n",
       " 'CD',\n",
       " ',',\n",
       " 'VBP',\n",
       " 'RB',\n",
       " 'VBN',\n",
       " '.',\n",
       " 'NNS',\n",
       " 'WDT',\n",
       " 'VBP',\n",
       " 'DT',\n",
       " 'NNS',\n",
       " 'IN',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'VBG',\n",
       " 'NNS',\n",
       " 'JJ',\n",
       " 'IN',\n",
       " 'NNS',\n",
       " 'CC',\n",
       " 'IN',\n",
       " 'JJ',\n",
       " 'NNS',\n",
       " 'MD',\n",
       " 'NN',\n",
       " 'TO',\n",
       " 'VB',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'PRP',\n",
       " 'MD',\n",
       " 'VB',\n",
       " 'VBG',\n",
       " 'NN',\n",
       " '.',\n",
       " 'IN',\n",
       " 'DT',\n",
       " ',',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NNS',\n",
       " 'MD',\n",
       " 'VB',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'CC',\n",
       " 'VB',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'VBZ',\n",
       " 'JJ',\n",
       " 'NNS',\n",
       " 'TO',\n",
       " 'VB',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'RB',\n",
       " 'RB',\n",
       " '.',\n",
       " 'VBG',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'MD',\n",
       " 'RB',\n",
       " 'VB',\n",
       " 'VBN',\n",
       " 'TO',\n",
       " 'VB',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'VBZ',\n",
       " 'VBN',\n",
       " 'DT',\n",
       " 'NNP',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'NN',\n",
       " ',',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'NNS',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'VBD',\n",
       " 'TO',\n",
       " 'VB',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'CD',\n",
       " 'NNS',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'NN',\n",
       " '.',\n",
       " 'IN',\n",
       " 'PRP$',\n",
       " 'NN',\n",
       " ',',\n",
       " 'DT',\n",
       " 'NNS',\n",
       " 'VBD',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'NN',\n",
       " 'NNS',\n",
       " 'CC',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'TO',\n",
       " 'VB',\n",
       " 'CC',\n",
       " 'VB',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'NNP',\n",
       " 'MD',\n",
       " 'VB',\n",
       " 'JJ',\n",
       " 'NNS',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NNS',\n",
       " 'WP',\n",
       " 'VBP',\n",
       " 'DT',\n",
       " 'NNS',\n",
       " ',',\n",
       " 'RB',\n",
       " 'RB',\n",
       " 'IN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'NNP',\n",
       " 'VBZ',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'CC',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'NNS',\n",
       " 'RB',\n",
       " '.',\n",
       " 'VB',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'WP',\n",
       " 'VBZ',\n",
       " ',',\n",
       " 'NN',\n",
       " 'NNS',\n",
       " 'VBP',\n",
       " ',',\n",
       " 'PRP',\n",
       " 'VBP',\n",
       " 'TO',\n",
       " 'VB',\n",
       " 'IN',\n",
       " 'CD',\n",
       " 'VBG',\n",
       " 'NNS',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " ',',\n",
       " 'CC',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'NN',\n",
       " 'TO',\n",
       " 'VB',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'PRP',\n",
       " 'VBZ',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'VBG',\n",
       " 'IN',\n",
       " 'VBG',\n",
       " 'NN',\n",
       " '.',\n",
       " 'NN',\n",
       " 'VBZ',\n",
       " 'VBP',\n",
       " 'IN',\n",
       " 'NN',\n",
       " 'RB',\n",
       " 'VBD',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NNS',\n",
       " ',',\n",
       " 'IN',\n",
       " 'VBN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " '.',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " ',',\n",
       " 'WDT',\n",
       " 'VBD',\n",
       " 'RB',\n",
       " 'VBN',\n",
       " 'IN',\n",
       " 'NN',\n",
       " 'DT',\n",
       " '.',\n",
       " 'NN',\n",
       " ',',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " ',',\n",
       " 'CC',\n",
       " 'RB',\n",
       " 'IN',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'NNS',\n",
       " ',',\n",
       " 'NNS',\n",
       " 'VBD',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " ',',\n",
       " 'VB',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " ',',\n",
       " 'VBD',\n",
       " 'VBN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'TO',\n",
       " 'VB',\n",
       " 'PRP',\n",
       " 'IN',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'VBD',\n",
       " 'JJ',\n",
       " '.',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'VBD',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'VBD',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'VBN',\n",
       " 'IN',\n",
       " ',',\n",
       " 'CC',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'VBD',\n",
       " 'VBN',\n",
       " 'RB',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'RB',\n",
       " 'VBD',\n",
       " 'CD',\n",
       " '.',\n",
       " 'RB',\n",
       " 'IN',\n",
       " 'JJ',\n",
       " 'TO',\n",
       " 'NN',\n",
       " 'NNS',\n",
       " ',',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'VBD',\n",
       " 'IN',\n",
       " 'NN',\n",
       " 'VBD',\n",
       " 'DT',\n",
       " 'VBG',\n",
       " 'TO',\n",
       " 'VB',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'DT',\n",
       " 'VBG',\n",
       " 'IN',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'NNS',\n",
       " 'VBD',\n",
       " 'VBD',\n",
       " 'VBN',\n",
       " 'CC',\n",
       " 'PRP',\n",
       " 'VBP',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'TO',\n",
       " 'VB',\n",
       " 'VBN',\n",
       " 'TO',\n",
       " 'VB',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " '.',\n",
       " 'CC',\n",
       " 'IN',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'CC',\n",
       " 'NN',\n",
       " ',',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'MD',\n",
       " 'VB',\n",
       " 'IN',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'NNS',\n",
       " 'IN',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'TO',\n",
       " 'VB',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'PRP$',\n",
       " 'VBN',\n",
       " 'NNS',\n",
       " 'IN',\n",
       " 'NN',\n",
       " 'NN',\n",
       " ',',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NNS',\n",
       " 'MD',\n",
       " 'VB',\n",
       " 'JJ',\n",
       " 'NNS',\n",
       " '.',\n",
       " 'PRP',\n",
       " 'VBZ',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'NNS',\n",
       " 'VBG',\n",
       " 'WRB',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'MD',\n",
       " 'VB',\n",
       " 'IN',\n",
       " 'NNS',\n",
       " 'WDT',\n",
       " 'VBZ',\n",
       " 'RB',\n",
       " 'VBN',\n",
       " 'WRB',\n",
       " 'PRP',\n",
       " 'VBD',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Duplcate and sort to list\n",
    "gtag = list(sorted(set(gtag))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"''\",\n",
       " ',',\n",
       " '.',\n",
       " 'CC',\n",
       " 'CD',\n",
       " 'DT',\n",
       " 'EX',\n",
       " 'FW',\n",
       " 'IN',\n",
       " 'JJ',\n",
       " 'JJR',\n",
       " 'JJS',\n",
       " 'LS',\n",
       " 'MD',\n",
       " 'NN',\n",
       " 'NNP',\n",
       " 'NNPS',\n",
       " 'NNS',\n",
       " 'PDT',\n",
       " 'POS',\n",
       " 'PRP',\n",
       " 'PRP$',\n",
       " 'RB',\n",
       " 'RBR',\n",
       " 'RBS',\n",
       " 'RP',\n",
       " 'TO',\n",
       " 'UH',\n",
       " 'VB',\n",
       " 'VBD',\n",
       " 'VBG',\n",
       " 'VBN',\n",
       " 'VBP',\n",
       " 'VBZ',\n",
       " 'WDT',\n",
       " 'WP',\n",
       " 'WP$',\n",
       " 'WRB']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 unique characters\n"
     ]
    }
   ],
   "source": [
    "print ('{} unique characters'.format(len(gtag)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flat list \n",
    "df2 =flatten(list(df2))\n",
    "#del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = {u:i for i, u in enumerate(gtag)}\n",
    "idx2char = np.array(gtag)\n",
    "text_as_int = np.array([char2idx[c] for c in df2 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(df2)//seq_length\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "#BATCH_SIZE = 64\n",
    "steps_per_epoch = examples_per_epoch//BATCH_SIZE\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "gtag_size = len(gtag)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.test.is_gpu_available():\n",
    "  rnn = tf.keras.layers.LSTM\n",
    "else:\n",
    "  import functools\n",
    "  rnn = functools.partial(\n",
    "    tf.keras.layers.GRU, recurrent_activation='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(gtag_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(gtag_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(embedding_dim, return_sequences=True),\n",
    "    rnn(rnn_units,\n",
    "        return_sequences=True,\n",
    "        recurrent_initializer='glorot_uniform',\n",
    "        stateful=True),\n",
    "    tf.keras.layers.Dense(gtag_size)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           9728      \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, None, 256)           394752    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (64, None, 1024)          5246976   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 38)            38950     \n",
      "=================================================================\n",
      "Total params: 5,690,406\n",
      "Trainable params: 5,690,406\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\n",
    "  gtag_size = len(gtag),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)\n",
    "optimizer = tf.optimizers.Adam()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)\n",
    "model = build_model(\n",
    "  gtag_size = len(gtag),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)\n",
    "optimizer = tf.optimizers.Adam()\n",
    "# Training step\n",
    "EPOCHS = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.90842557 4.37709904 2.10653925 ... 1.43870544 1.93942022 0.529133558]\n",
      " [1.49883044 6.66559315 0.204361796 ... 0.624088585 1.13244867 2.40866089]\n",
      " [1.94771826 2.32600522 2.6544385 ... 0.504787445 4.49381685 0.18744044]\n",
      " ...\n",
      " [0.498026 2.31765318 2.16876745 ... 3.41884804 2.13615894 0.415382385]\n",
      " [2.18814898 1.58397019 2.00100422 ... 1.26490355 1.9823184 2.02643442]\n",
      " [2.80072927 1.64817965 1.52534926 ... 1.3414191 2.22470665 1.91619849]]\n",
      "Epoch 1 Loss None\n",
      "Time taken for 1 epoch 16.796190977096558 sec\n",
      "\n",
      "[[2.9061265 4.75016975 2.13660336 ... 1.52751684 1.97805071 0.579254031]\n",
      " [1.6903981 6.40839386 0.184704036 ... 0.713359952 1.19537342 2.13749599]\n",
      " [1.87334204 2.32359362 2.50568533 ... 0.405967027 4.02941608 0.130836144]\n",
      " ...\n",
      " [0.521561563 2.66456652 2.48651481 ... 3.22939396 1.71253693 0.237276331]\n",
      " [1.72833598 1.38191378 2.06617379 ... 1.15458047 2.45782304 2.07348871]\n",
      " [2.04576349 1.50907087 1.84642828 ... 1.18026662 2.27710485 1.92551613]]\n",
      "Epoch 2 Loss None\n",
      "Time taken for 1 epoch 16.612001419067383 sec\n",
      "\n",
      "[[2.89789844 4.73452806 2.13669157 ... 1.38573015 1.96551776 0.602421403]\n",
      " [1.70323503 6.28906202 0.138688356 ... 0.782376289 1.10787988 2.0405364]\n",
      " [1.84317839 2.19713688 2.5506587 ... 0.426673174 4.44198132 0.139968708]\n",
      " ...\n",
      " [0.580361068 2.71987534 2.45452094 ... 3.21985626 1.55585682 0.132131264]\n",
      " [1.7555275 1.27997589 2.21373034 ... 1.0263406 2.61322379 2.12598324]\n",
      " [2.0280757 1.54512763 1.79566121 ... 1.14389193 2.38720202 2.1478436]]\n",
      "Epoch 3 Loss None\n",
      "Time taken for 1 epoch 16.44443655014038 sec\n",
      "\n",
      "[[3.01614904 4.72475815 2.18706918 ... 1.29218006 1.9762665 0.584090829]\n",
      " [1.70074499 6.24212027 0.102422342 ... 0.794559777 1.05738211 1.86659157]\n",
      " [1.76809621 2.13562679 2.52232814 ... 0.423586696 4.57744122 0.179181173]\n",
      " ...\n",
      " [0.585623741 2.67464638 2.38004065 ... 3.15915179 1.31949031 0.0913117379]\n",
      " [1.78640294 1.28489351 2.33039427 ... 1.07147288 2.51973057 2.06097579]\n",
      " [2.11319542 1.52853703 1.64788294 ... 1.09790826 2.57720208 2.33473611]]\n",
      "Epoch 4 Loss None\n",
      "Time taken for 1 epoch 16.625003576278687 sec\n",
      "\n",
      "[[3.10164428 4.56993246 2.27982163 ... 1.06236613 1.98547244 0.56885618]\n",
      " [1.76070619 6.32021618 0.0790302 ... 0.818678617 1.06226516 1.69705594]\n",
      " [1.69406283 2.05788922 2.45410657 ... 0.468365133 4.61133575 0.17664057]\n",
      " ...\n",
      " [0.58725 2.73847198 2.28069592 ... 3.224334 1.11635613 0.075196445]\n",
      " [1.74539101 1.28865564 2.43429136 ... 0.990763128 2.53431177 2.02956963]\n",
      " [2.20624208 1.5625329 1.6346668 ... 1.06052923 2.55309725 2.35950518]]\n",
      "Epoch 5 Loss None\n",
      "Time taken for 1 epoch 16.28899621963501 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 2070 super \n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    # initializing the hidden state at the start of every epoch\n",
    "    # initially hidden is None\n",
    "    hidden = model.reset_states()\n",
    "\n",
    "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
    "          with tf.GradientTape() as tape:\n",
    "              # feeding the hidden state back into the model\n",
    "              # This is the interesting step\n",
    "              predictions = model(inp)\n",
    "               \n",
    "              loss = tf.nn.sparse_softmax_cross_entropy_with_logits(target, predictions)\n",
    "\n",
    "          grads = tape.gradient(loss, model.trainable_variables)\n",
    "          optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "          if batch_n % 100 == 0:\n",
    "               pass\n",
    "               # print('Epoch {0} Batch {1} Loss:{2}'.format(epoch+1, batch_n, tf.print(loss)[0]))\n",
    "                \n",
    "               \n",
    "\n",
    "    # saving (checkpoint) the model every 5 epochs\n",
    "    if (epoch ) % 10 == 0:\n",
    "      model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
    "    if (epoch ) % 1000 == 0:\n",
    "      model.save_weights(checkpoint_prefix.format(epoch=epoch))    \n",
    "    print ('Epoch {} Loss {}'.format(epoch+1, tf.print(loss) ))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "\n",
    "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (1, None, 256)            9728      \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (1, None, 256)            394752    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (1, None, 1024)           5246976   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (1, None, 38)             38950     \n",
      "=================================================================\n",
      "Total params: 5,690,406\n",
      "Trainable params: 5,690,406\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)\n",
    "#tf.train.load_checkpoint(checkpoint_dir+\"/ckpt_150.index\")\n",
    "model = build_model(gtag_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
